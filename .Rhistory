cat("\n--- By Label and Time ---\n")
raw_data %>%
group_by(label, Time_factor) %>%
summarise(
M = mean(Rating),
SD = sd(Rating),
n = n(),
.groups = "drop"
) %>%
pivot_wider(
names_from = Time_factor,
values_from = c(M, SD, n),
names_sep = "_"
) %>%
print(n = 10)
# Use numeric Time for random slope (0/1)
raw_data$Time_numeric <- as.numeric(raw_data$Time_factor) - 1  # 0 for Pre, 1 for Post
accuracy_model_1b <- lmer(
# Rating ~ Time_factor * label + (1 + Time_numeric | Student) + (1 | Item_unique),
Rating ~ Time_factor * label + (1 | Student) + (1 | Item_unique),
data = raw_data,
REML = TRUE,
control = lmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 100000))
)
print(summary(accuracy_model_1b))
# Type III Tests of Fixed Effects
print(anova(accuracy_model_1b, type = 3))
# Variance Components
vc <- as.data.frame(VarCorr(accuracy_model_1b))
print(vc)
# ICC by grouping factor
# print(performance::icc(accuracy_model_1b, by_group = TRUE))
# Estimated marginal means
emm_accuracy <- emmeans(accuracy_model_1b, ~ Time_factor | label)
# Estimated Marginal Means by Time and Label
print(summary(emm_accuracy))
# Simple effects: Post - Pre within each label
accuracy_contrasts <- contrast(emm_accuracy, method = "pairwise")
accuracy_contrasts_summary <- summary(accuracy_contrasts, infer = TRUE)
print(accuracy_contrasts_summary)
# With corrections - With Bonferroni Correction
# print(summary(contrast(emm_accuracy, method = "pairwise", adjust = "bonferroni"), infer = TRUE))
# Compute the contrasts (still grouped by label)
emm_accuracy <- emmeans(accuracy_model_1b, ~ Time_factor | label)
accuracy_contrasts <- contrast(emm_accuracy, method = "pairwise")
# Apply Bonferroni across ALL 10 contrasts
summary(accuracy_contrasts, by = NULL, adjust = "bonferroni", infer = TRUE)
# Apply Holm across ALL 10 contrasts (probably better but doesn't make a difference here)
# summary(accuracy_contrasts, by = NULL, adjust = "holm", infer = TRUE)
# With corrections - With Bonferroni Correction
# print(summary(contrast(emm_accuracy, method = "pairwise", adjust = "bonferroni"), infer = TRUE))
# Compute the contrasts (still grouped by label)
emm_accuracy <- emmeans(accuracy_model_1b, ~ Time_factor | label)
accuracy_contrasts <- contrast(emm_accuracy, method = "pairwise")
# Apply Bonferroni across ALL 10 contrasts
summary(accuracy_contrasts, by = NULL, adjust = "bonferroni", infer = TRUE)
# Apply Holm across ALL 10 contrasts (probably better but doesn't make a difference here)
summary(accuracy_contrasts, by = NULL, adjust = "holm", infer = TRUE)
library(tidyverse)
library(lme4)
library(lmerTest)    # For p-values in lmer output
library(emmeans)     # For post-hoc comparisons
library(performance) # For model diagnostics
raw_data = read_csv("topic-closers.csv")
# Create Item_within_label (1-5 for each label)
# The current Item numbers are unique across labels (1-5 for prediction,
# 6-10 for summaries, etc.), so we need to renumber them 1-5 within each label.
raw_data <- raw_data %>%
group_by(label) %>%
mutate(
# Get the minimum item number for each label and subtract to get 1-5
Item_within_label = Item - min(Item) + 1
) %>%
ungroup()
# Now create a unique item identifier that distinguishes items across label × time
# Since items at Time 0 ≠ items at Time 1 (different sentences), we include Time
raw_data <- raw_data %>%
mutate(
# Unique item ID: combines label, time, and item position
Item_unique = paste(label, Time, Item_within_label, sep = "_"),
# Factor versions for modeling
Student = factor(Student),
label = factor(label),
Time_factor = factor(Time, levels = c(0, 1), labels = c("Pre", "Post")),
Item_unique = factor(Item_unique)
)
# Overall by Time
cat("--- Overall by Time ---\n")
raw_data %>%
group_by(Time_factor) %>%
summarise(
M = mean(Rating),
SD = sd(Rating),
n = n()
) %>%
print()
# By Label and Time
cat("\n--- By Label and Time ---\n")
raw_data %>%
group_by(label, Time_factor) %>%
summarise(
M = mean(Rating),
SD = sd(Rating),
n = n(),
.groups = "drop"
) %>%
pivot_wider(
names_from = Time_factor,
values_from = c(M, SD, n),
names_sep = "_"
) %>%
print(n = 10)
# Use numeric Time for random slope (0/1)
raw_data$Time_numeric <- as.numeric(raw_data$Time_factor) - 1  # 0 for Pre, 1 for Post
accuracy_model_1b <- lmer(
# Rating ~ Time_factor * label + (1 + Time_numeric | Student) + (1 | Item_unique),
Rating ~ Time_factor * label + (1 | Student) + (1 | Item_unique),
data = raw_data,
REML = TRUE,
control = lmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 100000))
)
print(summary(accuracy_model_1b))
# Type III Tests of Fixed Effects
print(anova(accuracy_model_1b, type = 3))
# Variance Components
vc <- as.data.frame(VarCorr(accuracy_model_1b))
print(vc)
# ICC by grouping factor
# print(performance::icc(accuracy_model_1b, by_group = TRUE))
# Estimated marginal means
emm_accuracy <- emmeans(accuracy_model_1b, ~ Time_factor | label)
# Estimated Marginal Means by Time and Label
print(summary(emm_accuracy))
# Simple effects: Post - Pre within each label
accuracy_contrasts <- contrast(emm_accuracy, method = "pairwise")
accuracy_contrasts_summary <- summary(accuracy_contrasts, infer = TRUE)
print(accuracy_contrasts_summary)
# With corrections - With Bonferroni Correction
# print(summary(contrast(emm_accuracy, method = "pairwise", adjust = "bonferroni"), infer = TRUE))
# Compute the contrasts (still grouped by label)
emm_accuracy <- emmeans(accuracy_model_1b, ~ Time_factor | label)
accuracy_contrasts <- contrast(emm_accuracy, method = "pairwise")
# Apply Bonferroni across ALL 10 contrasts
summary(accuracy_contrasts, by = NULL, adjust = "bonferroni", infer = TRUE)
# Apply Holm across ALL 10 contrasts (probably better but doesn't make a difference here)
# summary(accuracy_contrasts, by = NULL, adjust = "holm", infer = TRUE)
# Create aggregated data with Accuracy and Inconsistency
summary_data <- raw_data %>%
group_by(Student, label, Time, Time_factor) %>%
summarise(
Accuracy = mean(Rating, na.rm = TRUE),
Inconsistency = mean(abs(Rating - mean(Rating, na.rm = TRUE)), na.rm = TRUE),
n_items = n(),
.groups = "drop"
)
# Testing whether inconsistency decreases uniformly across labels
inconsistency_model_additive <- lmer(
Inconsistency ~ Time_factor + label + (1 | Student),
data = summary_data,
REML = TRUE
)
print(summary(inconsistency_model_additive))
# Type III Tests
print(anova(inconsistency_model_additive, type = 3))
# Test whether Time × Label Interaction is needed
inconsistency_model_interaction <- lmer(
Inconsistency ~ Time_factor * label + (1 | Student),
data = summary_data,
REML = TRUE
)
# Model comparison (additive vs. interaction)
print(anova(inconsistency_model_additive, inconsistency_model_interaction))
# Estimated Marginal Means for Time
emm_inconsistency <- emmeans(inconsistency_model_additive, ~ Time_factor)
print(summary(emm_inconsistency))
# Post minus Pre Contrast
print(contrast(emm_inconsistency, method = "pairwise", infer = TRUE))
# Effect Size
print(eff_size(emm_inconsistency, sigma = sigma(inconsistency_model_additive),
edf = df.residual(inconsistency_model_additive)))
# Table 1: Descriptive statistics by Label and Time
table1 <- raw_data %>%
group_by(label, Time_factor) %>%
summarise(
Rating_M = round(mean(Rating), 2),
Rating_SD = round(sd(Rating), 2),
.groups = "drop"
) %>%
pivot_wider(
names_from = Time_factor,
values_from = c(Rating_M, Rating_SD)
)
table1
# Table 2: Post-hoc results for accuracy
cat("\n--- Table 2: Accuracy Change by Label (from mixed model) ---\n")
posthoc_table <- as.data.frame(accuracy_contrasts_summary) %>%
mutate(
label = as.character(label),
estimate = round(estimate, 3),
SE = round(SE, 3),
t = round(t.ratio, 2),
p = round(p.value, 4),
sig = case_when(
p.value < 0.001 ~ "***",
p.value < 0.01 ~ "**",
p.value < 0.05 ~ "*",
p.value < 0.10 ~ "†",
TRUE ~ ""
)
) %>%
select(label, estimate, SE, t, p, sig)
posthoc_table
# Variance Components (Accuracy Model)
var_table <- as.data.frame(VarCorr(accuracy_model_1b)) %>%
mutate(
Variance = round(vcov, 3),
SD = round(sdcor, 3),
Percent = round(vcov / sum(vcov) * 100, 1)
) %>%
select(grp, Variance, SD, Percent)
var_table
# Accuracy Model Diagnostics
accuracy_model_1b_alt <- lmer(
Rating ~ Time_numeric * label + (1 + Time_numeric | Student) + (1 | Item_unique),
data = raw_data,
REML = TRUE,
control = lmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 100000))
)
performance::r2_nakagawa(accuracy_model_1b_alt)
# Inconsistency Model Diagnostics
# "R² (marginal):
round(performance::r2_nakagawa(inconsistency_model_additive)$R2_marginal, 3)
# "R² (conditional):
round(performance::r2_nakagawa(inconsistency_model_additive)$R2_conditional, 3)
raw_data <- read_csv("topic_closers_item_level.csv")
raw_data <- raw_data %>%
mutate(
Effectiveness = case_when(
label %in% c("SimpleConclusions", "ComplexConclusions", "summaries",
"prediction", "recommendations", "Speculations") ~ "Effective",
label %in% c("Supplementals", "Support") ~ "Underrecognized",
label %in% c("Elaborations", "topicsentences") ~ "Ineffective"
),
# Factor with theoretically meaningful ordering
Effectiveness = factor(Effectiveness,
levels = c("Effective", "Underrecognized", "Ineffective"))
)
raw_data %>%
distinct(label, Effectiveness) %>%
arrange(Effectiveness, label) %>%
print(n = 10)
raw_data %>%
count(Effectiveness) %>%
print()
effectiveness_desc <- raw_data %>%
group_by(Effectiveness, Time_factor) %>%
summarise(
M = mean(Rating),
SD = sd(Rating),
n = n(),
.groups = "drop"
)
effectiveness_desc
# Primary model with crossed random effects
# Note: Item_unique is still nested within Label (and thus within Effectiveness)
effectiveness_model <- lmer(
# Rating ~ Time_factor * Effectiveness + (1 + Time_numeric | Student) + (1 | Item_unique),
Rating ~ Time_factor * Effectiveness + (1 | Student) + (1 | Item_unique),
data = raw_data,
REML = TRUE,
control = lmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 100000))
)
print(summary(effectiveness_model))
### Type III Tests of Fixed Effects
print(anova(effectiveness_model, type = 3))
# Estimated marginal means
emm_eff <- emmeans(effectiveness_model, ~ Time_factor | Effectiveness)
# Estimated Marginal Means
print(summary(emm_eff))
# Simple effects: Post - Pre within each Effectiveness group
eff_contrasts <- contrast(emm_eff, method = "pairwise")
print(summary(eff_contrasts, infer = TRUE))
# Effect sizes
print(eff_size(emm_eff, sigma = sigma(effectiveness_model),
edf = df.residual(effectiveness_model)))
library(tidyverse)
library(lme4)
library(lmerTest)    # For p-values in lmer output
library(emmeans)     # For post-hoc comparisons
library(performance) # For model diagnostics
raw_data = read_csv("topic-closers.csv")
# Create Item_within_label (1-5 for each label)
# The current Item numbers are unique across labels (1-5 for prediction,
# 6-10 for summaries, etc.), so we need to renumber them 1-5 within each label.
raw_data <- raw_data %>%
group_by(label) %>%
mutate(
# Get the minimum item number for each label and subtract to get 1-5
Item_within_label = Item - min(Item) + 1
) %>%
ungroup()
# Now create a unique item identifier that distinguishes items across label × time
# Since items at Time 0 ≠ items at Time 1 (different sentences), we include Time
raw_data <- raw_data %>%
mutate(
# Unique item ID: combines label, time, and item position
Item_unique = paste(label, Time, Item_within_label, sep = "_"),
# Factor versions for modeling
Student = factor(Student),
label = factor(label),
Time_factor = factor(Time, levels = c(0, 1), labels = c("Pre", "Post")),
Item_unique = factor(Item_unique)
)
# Overall by Time
cat("--- Overall by Time ---\n")
raw_data %>%
group_by(Time_factor) %>%
summarise(
M = mean(Rating),
SD = sd(Rating),
n = n()
) %>%
print()
# By Label and Time
cat("\n--- By Label and Time ---\n")
raw_data %>%
group_by(label, Time_factor) %>%
summarise(
M = mean(Rating),
SD = sd(Rating),
n = n(),
.groups = "drop"
) %>%
pivot_wider(
names_from = Time_factor,
values_from = c(M, SD, n),
names_sep = "_"
) %>%
print(n = 10)
# Use numeric Time for random slope (0/1)
raw_data$Time_numeric <- as.numeric(raw_data$Time_factor) - 1  # 0 for Pre, 1 for Post
accuracy_model_1b <- lmer(
# Rating ~ Time_factor * label + (1 + Time_numeric | Student) + (1 | Item_unique),
Rating ~ Time_factor * label + (1 | Student) + (1 | Item_unique),
data = raw_data,
REML = TRUE,
control = lmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 100000))
)
print(summary(accuracy_model_1b))
# Type III Tests of Fixed Effects
print(anova(accuracy_model_1b, type = 3))
# Variance Components
vc <- as.data.frame(VarCorr(accuracy_model_1b))
print(vc)
# ICC by grouping factor
# print(performance::icc(accuracy_model_1b, by_group = TRUE))
# Estimated marginal means
emm_accuracy <- emmeans(accuracy_model_1b, ~ Time_factor | label)
# Estimated Marginal Means by Time and Label
print(summary(emm_accuracy))
# Simple effects: Post - Pre within each label
accuracy_contrasts <- contrast(emm_accuracy, method = "pairwise")
accuracy_contrasts_summary <- summary(accuracy_contrasts, infer = TRUE)
print(accuracy_contrasts_summary)
# With corrections - With Bonferroni Correction
# print(summary(contrast(emm_accuracy, method = "pairwise", adjust = "bonferroni"), infer = TRUE))
# Compute the contrasts (still grouped by label)
emm_accuracy <- emmeans(accuracy_model_1b, ~ Time_factor | label)
accuracy_contrasts <- contrast(emm_accuracy, method = "pairwise")
# Apply Bonferroni across ALL 10 contrasts
summary(accuracy_contrasts, by = NULL, adjust = "bonferroni", infer = TRUE)
# Apply Holm across ALL 10 contrasts (probably better but doesn't make a difference here)
# summary(accuracy_contrasts, by = NULL, adjust = "holm", infer = TRUE)
# Create aggregated data with Accuracy and Inconsistency
summary_data <- raw_data %>%
group_by(Student, label, Time, Time_factor) %>%
summarise(
Accuracy = mean(Rating, na.rm = TRUE),
Inconsistency = mean(abs(Rating - mean(Rating, na.rm = TRUE)), na.rm = TRUE),
n_items = n(),
.groups = "drop"
)
# Testing whether inconsistency decreases uniformly across labels
inconsistency_model_additive <- lmer(
Inconsistency ~ Time_factor + label + (1 | Student),
data = summary_data,
REML = TRUE
)
print(summary(inconsistency_model_additive))
# Type III Tests
print(anova(inconsistency_model_additive, type = 3))
# Test whether Time × Label Interaction is needed
inconsistency_model_interaction <- lmer(
Inconsistency ~ Time_factor * label + (1 | Student),
data = summary_data,
REML = TRUE
)
# Model comparison (additive vs. interaction)
print(anova(inconsistency_model_additive, inconsistency_model_interaction))
# Estimated Marginal Means for Time
emm_inconsistency <- emmeans(inconsistency_model_additive, ~ Time_factor)
print(summary(emm_inconsistency))
# Post minus Pre Contrast
print(contrast(emm_inconsistency, method = "pairwise", infer = TRUE))
# Effect Size
print(eff_size(emm_inconsistency, sigma = sigma(inconsistency_model_additive),
edf = df.residual(inconsistency_model_additive)))
# Table 1: Descriptive statistics by Label and Time
table1 <- raw_data %>%
group_by(label, Time_factor) %>%
summarise(
Rating_M = round(mean(Rating), 2),
Rating_SD = round(sd(Rating), 2),
.groups = "drop"
) %>%
pivot_wider(
names_from = Time_factor,
values_from = c(Rating_M, Rating_SD)
)
table1
# Table 2: Post-hoc results for accuracy
cat("\n--- Table 2: Accuracy Change by Label (from mixed model) ---\n")
posthoc_table <- as.data.frame(accuracy_contrasts_summary) %>%
mutate(
label = as.character(label),
estimate = round(estimate, 3),
SE = round(SE, 3),
t = round(t.ratio, 2),
p = round(p.value, 4),
sig = case_when(
p.value < 0.001 ~ "***",
p.value < 0.01 ~ "**",
p.value < 0.05 ~ "*",
p.value < 0.10 ~ "†",
TRUE ~ ""
)
) %>%
select(label, estimate, SE, t, p, sig)
posthoc_table
# Variance Components (Accuracy Model)
var_table <- as.data.frame(VarCorr(accuracy_model_1b)) %>%
mutate(
Variance = round(vcov, 3),
SD = round(sdcor, 3),
Percent = round(vcov / sum(vcov) * 100, 1)
) %>%
select(grp, Variance, SD, Percent)
var_table
# Accuracy Model Diagnostics
accuracy_model_1b_alt <- lmer(
Rating ~ Time_numeric * label + (1 + Time_numeric | Student) + (1 | Item_unique),
data = raw_data,
REML = TRUE,
control = lmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 100000))
)
performance::r2_nakagawa(accuracy_model_1b_alt)
# Inconsistency Model Diagnostics
# "R² (marginal):
round(performance::r2_nakagawa(inconsistency_model_additive)$R2_marginal, 3)
# "R² (conditional):
round(performance::r2_nakagawa(inconsistency_model_additive)$R2_conditional, 3)
raw_data <- read_csv("topic_closers_item_level.csv")
raw_data <- raw_data %>%
mutate(
Effectiveness = case_when(
label %in% c("SimpleConclusions", "ComplexConclusions", "summaries",
"prediction", "recommendations", "Speculations") ~ "Effective",
label %in% c("Supplementals", "Support") ~ "Underrecognized",
label %in% c("Elaborations", "topicsentences") ~ "Ineffective"
),
# Factor with theoretically meaningful ordering
Effectiveness = factor(Effectiveness,
levels = c("Effective", "Underrecognized", "Ineffective"))
)
raw_data %>%
distinct(label, Effectiveness) %>%
arrange(Effectiveness, label) %>%
print(n = 10)
raw_data %>%
count(Effectiveness) %>%
print()
effectiveness_desc <- raw_data %>%
group_by(Effectiveness, Time_factor) %>%
summarise(
M = mean(Rating),
SD = sd(Rating),
n = n(),
.groups = "drop"
)
effectiveness_desc
# Primary model with crossed random effects
# Note: Item_unique is still nested within Label (and thus within Effectiveness)
effectiveness_model <- lmer(
# Rating ~ Time_factor * Effectiveness + (1 + Time_numeric | Student) + (1 | Item_unique),
Rating ~ Time_factor * Effectiveness + (1 | Student) + (1 | Item_unique),
data = raw_data,
REML = TRUE,
control = lmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 100000))
)
print(summary(effectiveness_model))
### Type III Tests of Fixed Effects
print(anova(effectiveness_model, type = 3))
# Estimated marginal means
emm_eff <- emmeans(effectiveness_model, ~ Time_factor | Effectiveness)
# Estimated Marginal Means
print(summary(emm_eff))
# Simple effects: Post - Pre within each Effectiveness group
eff_contrasts <- contrast(emm_eff, method = "pairwise")
print(summary(eff_contrasts, infer = TRUE))
# Effect sizes
print(eff_size(emm_eff, sigma = sigma(effectiveness_model),
edf = df.residual(effectiveness_model)))
