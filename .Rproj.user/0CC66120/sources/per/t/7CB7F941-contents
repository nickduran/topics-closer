---
title: "R Notebook"
output: html_notebook
---

```{r}
library(tidyverse)
library(lme4)
library(lmerTest)    # For p-values in lmer output
library(emmeans)     # For post-hoc comparisons
library(performance) # For model diagnostics
```

```{r}

raw_data = read_csv("topic-closers.csv")

```

# -----------------------------------------------------------------------------
# Create proper item identifiers for mixed effects nesting
# -----------------------------------------------------------------------------

CRITICAL: Items at Time 0 and Time 1 are DIFFERENT sentences, even though they share the same Item number within each label. We need unique identifiers.

```{r}

# Create Item_within_label (1-5 for each label)
# The current Item numbers are unique across labels (1-5 for prediction, 
# 6-10 for summaries, etc.), so we need to renumber them 1-5 within each label.

raw_data <- raw_data %>%
  group_by(label) %>%
  mutate(
    # Get the minimum item number for each label and subtract to get 1-5
    Item_within_label = Item - min(Item) + 1
  ) %>%
  ungroup()

# Now create a unique item identifier that distinguishes items across label × time
# Since items at Time 0 ≠ items at Time 1 (different sentences), we include Time
raw_data <- raw_data %>%
  mutate(
    # Unique item ID: combines label, time, and item position
    Item_unique = paste(label, Time, Item_within_label, sep = "_"),
    
    # Factor versions for modeling
    Student = factor(Student),
    label = factor(label),
    Time_factor = factor(Time, levels = c(0, 1), labels = c("Pre", "Post")),
    Item_unique = factor(Item_unique)
  )

```

# -----------------------------------------------------------------------------
# Descriptive statistics
# -----------------------------------------------------------------------------

```{r}
# Overall by Time
cat("--- Overall by Time ---\n")
raw_data %>%
  group_by(Time_factor) %>%
  summarise(
    M = mean(Rating),
    SD = sd(Rating),
    n = n()
  ) %>%
  print()

# By Label and Time
cat("\n--- By Label and Time ---\n")
raw_data %>%
  group_by(label, Time_factor) %>%
  summarise(
    M = mean(Rating),
    SD = sd(Rating),
    n = n(),
    .groups = "drop"
  ) %>%
  pivot_wider(
    names_from = Time_factor,
    values_from = c(M, SD, n),
    names_sep = "_"
  ) %>%
  print(n = 10)
```

# -----------------------------------------------------------------------------
# ANALYSIS 1: Accuracy at Item Level
# -----------------------------------------------------------------------------

Design considerations:
- Students (22) are CROSSED with Label (10) and Time (2)
- Items are NESTED within Label × Time (each label-time cell has unique items)
- Each student rates each item once

Random effects structure:
- (1 | Student): Random intercept for students (some rate higher/lower overall)
- (1 | Item_unique): Random intercept for items (some items elicit higher/lower ratings)
<!-- - (1 + Time | Student): Random slope for time by student (individuals differ in learning) -->

### Model 

```{r}
# Use numeric Time for random slope (0/1)
raw_data$Time_numeric <- as.numeric(raw_data$Time_factor) - 1  # 0 for Pre, 1 for Post

accuracy_model_1b <- lmer(
  # Rating ~ Time_factor * label + (1 + Time_numeric | Student) + (1 | Item_unique),
  Rating ~ Time_factor * label + (1 | Student) + (1 | Item_unique),
  data = raw_data,
  REML = TRUE,
  control = lmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 100000))
)

```

### Model summary

```{r}

print(summary(accuracy_model_1b))

# Type III Tests of Fixed Effects
print(anova(accuracy_model_1b, type = 3))

# Variance Components 
vc <- as.data.frame(VarCorr(accuracy_model_1b))
print(vc)

# ICC by grouping factor
# print(performance::icc(accuracy_model_1b, by_group = TRUE))
```

### Post-hoc tests for accuracy

```{r}
# Estimated marginal means
emm_accuracy <- emmeans(accuracy_model_1b, ~ Time_factor | label)

# Estimated Marginal Means by Time and Label
print(summary(emm_accuracy))

# Simple effects: Post - Pre within each label
accuracy_contrasts <- contrast(emm_accuracy, method = "pairwise")
accuracy_contrasts_summary <- summary(accuracy_contrasts, infer = TRUE)
print(accuracy_contrasts_summary)

```
```{r}

# With corrections - With Bonferroni Correction
# print(summary(contrast(emm_accuracy, method = "pairwise", adjust = "bonferroni"), infer = TRUE))

# Compute the contrasts (still grouped by label)
emm_accuracy <- emmeans(accuracy_model_1b, ~ Time_factor | label)
accuracy_contrasts <- contrast(emm_accuracy, method = "pairwise")

# Apply Bonferroni across ALL 10 contrasts
summary(accuracy_contrasts, by = NULL, adjust = "bonferroni", infer = TRUE)

# Apply Holm across ALL 10 contrasts (probably better but doesn't make a difference here)
# summary(accuracy_contrasts, by = NULL, adjust = "holm", infer = TRUE)

```

# -----------------------------------------------------------------------------
# ANALYSIS 2: Inconsistency
# -----------------------------------------------------------------------------

Inconsistency is defined at the participant × label × time level (MAD of 5 items).
We are aggregating to participant × label × time, then model - inconsistency is conceptually an aggregate measure

```{r}
# Create aggregated data with Accuracy and Inconsistency
summary_data <- raw_data %>%
  group_by(Student, label, Time, Time_factor) %>%
  summarise(
    Accuracy = mean(Rating, na.rm = TRUE),
    Inconsistency = mean(abs(Rating - mean(Rating, na.rm = TRUE)), na.rm = TRUE),
    n_items = n(),
    .groups = "drop"
  )

```

```{r}
# Testing whether inconsistency decreases uniformly across labels

inconsistency_model_additive <- lmer(
  Inconsistency ~ Time_factor + label + (1 | Student),
  data = summary_data,
  REML = TRUE
)

print(summary(inconsistency_model_additive))

# Type III Tests 
print(anova(inconsistency_model_additive, type = 3))

# Test whether Time × Label Interaction is needed
inconsistency_model_interaction <- lmer(
  Inconsistency ~ Time_factor * label + (1 | Student),
  data = summary_data,
  REML = TRUE
)

# Model comparison (additive vs. interaction)
print(anova(inconsistency_model_additive, inconsistency_model_interaction))

# Estimated Marginal Means for Time
emm_inconsistency <- emmeans(inconsistency_model_additive, ~ Time_factor)
print(summary(emm_inconsistency))

# Post minus Pre Contrast
print(contrast(emm_inconsistency, method = "pairwise", infer = TRUE))

# Effect Size
print(eff_size(emm_inconsistency, sigma = sigma(inconsistency_model_additive), 
               edf = df.residual(inconsistency_model_additive)))
```

### Summary tables for manuscript

```{r}
# Table 1: Descriptive statistics by Label and Time
table1 <- raw_data %>%
  group_by(label, Time_factor) %>%
  summarise(
    Rating_M = round(mean(Rating), 2),
    Rating_SD = round(sd(Rating), 2),
    .groups = "drop"
  ) %>%
  pivot_wider(
    names_from = Time_factor,
    values_from = c(Rating_M, Rating_SD)
  )

table1

```

```{r}
# Table 2: Post-hoc results for accuracy
cat("\n--- Table 2: Accuracy Change by Label (from mixed model) ---\n")
posthoc_table <- as.data.frame(accuracy_contrasts_summary) %>%
  mutate(
    label = as.character(label),
    estimate = round(estimate, 3),
    SE = round(SE, 3),
    t = round(t.ratio, 2),
    p = round(p.value, 4),
    sig = case_when(
      p.value < 0.001 ~ "***",
      p.value < 0.01 ~ "**",
      p.value < 0.05 ~ "*",
      p.value < 0.10 ~ "†",
      TRUE ~ ""
    )
  ) %>%
  select(label, estimate, SE, t, p, sig)

posthoc_table

```

```{r}
# Variance Components (Accuracy Model)
var_table <- as.data.frame(VarCorr(accuracy_model_1b)) %>%
  mutate(
    Variance = round(vcov, 3),
    SD = round(sdcor, 3),
    Percent = round(vcov / sum(vcov) * 100, 1)
  ) %>%
  select(grp, Variance, SD, Percent)

var_table
```

### Model diagnostics

```{r}
# Accuracy Model Diagnostics 
accuracy_model_1b_alt <- lmer(
  Rating ~ Time_numeric * label + (1 + Time_numeric | Student) + (1 | Item_unique),
  data = raw_data,
  REML = TRUE,
  control = lmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 100000))
)

performance::r2_nakagawa(accuracy_model_1b_alt)
```

```{r}
# Inconsistency Model Diagnostics
# "R² (marginal):
round(performance::r2_nakagawa(inconsistency_model_additive)$R2_marginal, 3)
# "R² (conditional): 
round(performance::r2_nakagawa(inconsistency_model_additive)$R2_conditional, 3)

```

# -----------------------------------------------------------------------------
# ANALYSIS 3: MORE DIRECT ANALYSIS OF HYPOTHESIS FOR ACCURACY 
# -----------------------------------------------------------------------------

The manuscript hypothesizes that "effective" closers should be rated higher post-intervention while "ineffective" closers should stay low or decrease. But this is never directly tested.

Below directly tests the core theoretical hypothesis that learning will differentially affect ratings based on the pedagogical effectiveness of different closer types.

### Create Effectiveness Grouping Variable

Theoretical categories based on pedagogical framework:

EFFECTIVE (6 categories): Recognized as appropriate paragraph closers
 - SimpleConclusions, ComplexConclusions, summaries, 
 - prediction, recommendations, Speculations

UNDERRECOGNIZED (2 categories): Legitimate but students often miss these
 - Supplementals, Support

INEFFECTIVE (2 categories): Poor closers that should be avoided
 - Elaborations, topicsentences

```{r}

raw_data <- read_csv("topic_closers_item_level.csv")

raw_data <- raw_data %>%
  mutate(
    Effectiveness = case_when(
      label %in% c("SimpleConclusions", "ComplexConclusions", "summaries",
                   "prediction", "recommendations", "Speculations") ~ "Effective",
      label %in% c("Supplementals", "Support") ~ "Underrecognized",
      label %in% c("Elaborations", "topicsentences") ~ "Ineffective"
    ),
    # Factor with theoretically meaningful ordering
    Effectiveness = factor(Effectiveness, 
                           levels = c("Effective", "Underrecognized", "Ineffective"))
  )

raw_data %>%
  distinct(label, Effectiveness) %>%
  arrange(Effectiveness, label) %>%
  print(n = 10)

raw_data %>%
  count(Effectiveness) %>%
  print()

```
```{r}
effectiveness_desc <- raw_data %>%
  group_by(Effectiveness, Time_factor) %>%
  summarise(
    M = mean(Rating),
    SD = sd(Rating),
    n = n(),
    .groups = "drop"
  )
effectiveness_desc

```

### Mixed-Effects Model: Time × Effectiveness

```{r}
# Primary model with crossed random effects
# Note: Item_unique is still nested within Label (and thus within Effectiveness)

effectiveness_model <- lmer(
  # Rating ~ Time_factor * Effectiveness + (1 + Time_numeric | Student) + (1 | Item_unique),
  Rating ~ Time_factor * Effectiveness + (1 | Student) + (1 | Item_unique),
  data = raw_data,
  REML = TRUE,
  control = lmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 100000))
)

print(summary(effectiveness_model))

### Type III Tests of Fixed Effects
print(anova(effectiveness_model, type = 3))
```

### Post-Hoc Tests: Simple Effects of Time within Each Effectiveness Group

```{r}
# Estimated marginal means
emm_eff <- emmeans(effectiveness_model, ~ Time_factor | Effectiveness)

# Estimated Marginal Means 
print(summary(emm_eff))

# Simple effects: Post - Pre within each Effectiveness group
eff_contrasts <- contrast(emm_eff, method = "pairwise")
print(summary(eff_contrasts, infer = TRUE))

# Effect sizes
print(eff_size(emm_eff, sigma = sigma(effectiveness_model), 
               edf = df.residual(effectiveness_model)))
```

